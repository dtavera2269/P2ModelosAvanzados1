{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WXhwxdcYPa5G"
   },
   "source": [
    "# Project 3\n",
    "\n",
    "\n",
    "# Movie Genre Classification\n",
    "\n",
    "Classify a movie genre based on its plot.\n",
    "\n",
    "<img src=\"moviegenre.png\"\n",
    "     style=\"float: left; margin-right: 10px;\" />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "https://www.kaggle.com/c/miia4201-202019-p3-moviegenreclassification/overview\n",
    "\n",
    "### Data\n",
    "\n",
    "Input:\n",
    "- movie plot\n",
    "\n",
    "Output:\n",
    "Probability of the movie belong to each genre\n",
    "\n",
    "\n",
    "### Evaluation\n",
    "\n",
    "- 20% API\n",
    "- 30% Report with all the details of the solution, the analysis and the conclusions. The report cannot exceed 10 pages, must be send in PDF format and must be self-contained.\n",
    "- 50% Performance in the Kaggle competition (The grade for each group will be proportional to the ranking it occupies in the competition. The group in the first place will obtain 5 points, for each position below, 0.25 points will be subtracted, that is: first place: 5 points, second: 4.75 points, third place: 4.50 points ... eleventh place: 2.50 points, twelfth place: 2.25 points).\n",
    "\n",
    "• The project must be carried out in the groups assigned for module 4.\n",
    "• Use clear and rigorous procedures.\n",
    "• The delivery of the project is on July 12, 2020, 11:59 pm, through Sicua + (Upload: the API and the report in PDF format).\n",
    "• No projects will be received after the delivery time or by any other means than the one established. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Acknowledgements\n",
    "\n",
    "We thank Professor Fabio Gonzalez, Ph.D. and his student John Arevalo for providing this dataset.\n",
    "\n",
    "See https://arxiv.org/abs/1702.01992"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OuDHe5DnPa5J"
   },
   "source": [
    "## Sample Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oP3rkto9Pa5L"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.metrics import r2_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-fIsWzcaPa5X"
   },
   "outputs": [],
   "source": [
    "dataTraining = pd.read_csv('https://github.com/albahnsen/AdvancedMethodsDataAnalysisClass/raw/master/datasets/dataTraining.zip', encoding='UTF-8', index_col=0)\n",
    "dataTesting = pd.read_csv('https://github.com/albahnsen/AdvancedMethodsDataAnalysisClass/raw/master/datasets/dataTesting.zip', encoding='UTF-8', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mMmyABVWPa5r"
   },
   "source": [
    "#### Arreglo a la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CF0B6XE8Pa5s"
   },
   "outputs": [],
   "source": [
    "def arregla_doble_espacio(sentence):\n",
    "    cleaned_sentence = re.sub(r\" '\",\"'\", sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "def arregla_espacio2(sentence):\n",
    "    cleaned_sentence = re.sub(r\"' \",\"'\", sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "def arregla_abrev(sentence):\n",
    "    cleaned_sentence = re.sub(r\"won't\",\" will not\", sentence)\n",
    "    cleaned_sentence = re.sub(r\"can\\'t\",\" can not\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"n\\'t\",\" not\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"\\'re\",\" are\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"\\'s\",\" is\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"\\'d\",\" would\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"\\'ll\",\" will\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"\\'t\",\" not\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"\\'ve\",\" have\", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\"\\'m\",\" am\", cleaned_sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "def remove_punctuation(sentence):\n",
    "    cleaned_sentence = re.sub(r'[?|!|\\'|\"|#]', '', sentence)\n",
    "    cleaned_sentence = re.sub(r'[,|.|;|:|(|)|{|}|\\|/|<|>]|-|=|&|%', ' ', cleaned_sentence)\n",
    "    cleaned_sentence = cleaned_sentence.replace(\"\\n\",\" \")\n",
    "    cleaned_sentence = re.sub(' +', ' ', cleaned_sentence)\n",
    "    return cleaned_sentence\n",
    "\n",
    "def stopwords_a_mano(sentence):\n",
    "    cleaned_sentence = re.sub(r\" the \",\" \", sentence)\n",
    "    cleaned_sentence = re.sub(r\" is \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" of \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" a \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" his \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" to \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" at \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" it \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" he \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" her \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" in \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" she \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" an \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" whit \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" by \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" him \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" for \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" that \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" and \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" are \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" not \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" on \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" this \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" with \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" who \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" where \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" into \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" a$$ \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" a&m \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" aa \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" aaa \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" ¡olé \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" £ \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" °f \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" ½ \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" è \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" òs \",\" \", cleaned_sentence)\n",
    "    cleaned_sentence = re.sub(r\" ôishi \",\" \", cleaned_sentence)\n",
    "\n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Cm8-l8_Pa50"
   },
   "outputs": [],
   "source": [
    "dataTraining['plot'] = dataTraining['plot'].apply(arregla_doble_espacio)\n",
    "dataTraining['plot'] = dataTraining['plot'].apply(arregla_espacio2)\n",
    "dataTraining['plot'] = dataTraining['plot'].apply(arregla_abrev)\n",
    "dataTraining['plot'] = dataTraining['plot'].apply(remove_punctuation)\n",
    "dataTraining['plot'] = dataTraining['plot'].apply(stopwords_a_mano)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RdUeTlNCPa57"
   },
   "outputs": [],
   "source": [
    "dataTesting['plot'] = dataTesting['plot'].apply(arregla_doble_espacio)\n",
    "dataTesting['plot'] = dataTesting['plot'].apply(arregla_espacio2)\n",
    "dataTesting['plot'] = dataTesting['plot'].apply(arregla_abrev)\n",
    "dataTesting['plot'] = dataTesting['plot'].apply(remove_punctuation)\n",
    "dataTesting['plot'] = dataTesting['plot'].apply(stopwords_a_mano)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I4pE4S3bPa7i"
   },
   "source": [
    "### Create y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G06lZQscPa7k",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataTraining['genres'] = dataTraining['genres'].map(lambda x: eval(x))\n",
    "le = MultiLabelBinarizer()\n",
    "y_genres = le.fit_transform(dataTraining['genres'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l_3GeAJFPa6X"
   },
   "source": [
    "## **MLP**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 83
    },
    "colab_type": "code",
    "id": "wHkacxxEQWmc",
    "outputId": "c60a3f78-7a8f-4956-afe6-29fef8a74998"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\david\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hHhwQ7GcQfPi"
   },
   "outputs": [],
   "source": [
    "# define a function that accepts text and returns a list of lemmas\n",
    "def split_into_lemmas(text):\n",
    "    porter = PorterStemmer()\n",
    "    text = text.lower()\n",
    "    words = text.split()\n",
    "    words = [porter.stem(word) for word in words]\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return [wordnet_lemmatizer.lemmatize(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "at02n0txQipq"
   },
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "vect = CountVectorizer(stop_words='english', max_features=4500, max_df = 0.8)\n",
    "X_dtm = vect.fit_transform(dataTraining['plot'])\n",
    "words = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "FXkjhHCD1s4F",
    "outputId": "0fdee9df-a2b1-4935-a473-1d7c2213e88d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4500"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vocabulario\n",
    "X = dataTraining['plot']\n",
    "vocabulary = {x: idx + 1 for idx, x in enumerate(set(words))}\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "phqupcE8DMgU",
    "outputId": "02b9222b-b5d9-482c-a42a-583d1d1d23f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5921, 4500) (1974, 4500) (5921, 24) (1974, 24)\n"
     ]
    }
   ],
   "source": [
    "#Divido la base de datos\n",
    "X_train, X_test, y_train_genres, y_test_genres = train_test_split(X_dtm, y_genres, test_size=0.25, random_state=42)\n",
    "print(X_train.shape, X_test.shape, y_train_genres.shape, y_test_genres.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 442
    },
    "colab_type": "code",
    "id": "hpcdTODKDdWO",
    "outputId": "a20682b5-de1c-436e-a935-f87d4ab506da"
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "clf = OneVsRestClassifier(RandomForestClassifier(n_jobs=-1, n_estimators=1000, max_depth=None, random_state=42, min_samples_split = 2, criterion='entropy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "QRTGVdKnDeGl",
    "outputId": "253f75e8-871d-4229-8129-5b8d81ed358e"
   },
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "QRTGVdKnDeGl",
    "outputId": "253f75e8-871d-4229-8129-5b8d81ed358e"
   },
   "outputs": [],
   "source": [
    "y_pred_genres = clf.predict_proba(X_test)\n",
    "roc_auc_score(y_test_genres, y_pred_genres, average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(clf, 'model_deployment_movie/movie.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joblib.dump(vect, 'model_deployment_movie/vect2.pkl', compress=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcoronelv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jcoronelv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jcoronelv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from model_deployment_movie.m10 import valores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a serial killer decides to teach the secrets of his satisfying career to a video store clerk .'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot1 = dataTraining['plot'].iloc[1]\n",
    "plot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p_Documentary'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valores(plot1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prueba (plot1):\n",
    "\n",
    "    cols = ['p_Action', 'p_Adventure', 'p_Animation', 'p_Biography', 'p_Comedy', 'p_Crime', 'p_Documentary', 'p_Drama', 'p_Family',\n",
    "            'p_Fantasy', 'p_Film-Noir', 'p_History', 'p_Horror', 'p_Music', 'p_Musical', 'p_Mystery', 'p_News', 'p_Romance',\n",
    "            'p_Sci-Fi', 'p_Short', 'p_Sport', 'p_Thriller', 'p_War', 'p_Western']\n",
    "\n",
    "    X_test_dtm = vect.transform(['plot1'])\n",
    "\n",
    "    y_pred_test_genres = clf.predict_proba(X_test_dtm)\n",
    "\n",
    "    res = pd.DataFrame(y_pred_test_genres,columns=cols)\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask\n",
    "from flask_restplus import Api, Resource, fields\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "\n",
    "api = Api(\n",
    "    app, \n",
    "    version='1.0', \n",
    "    title='Movie Prediction genre',\n",
    "    description='Movie Prediction genre')\n",
    "\n",
    "ns = api.namespace('valores', \n",
    "     description='Movie Prediction genre')\n",
    "   \n",
    "parser = api.parser()\n",
    "\n",
    "parser.add_argument(\n",
    "    'plot', \n",
    "    type=str, \n",
    "    required=True, \n",
    "    help='plot movie to be analyzed', \n",
    "    location='args')\n",
    "\n",
    "\n",
    "\n",
    "resource_fields = api.model('Resource', {\n",
    "    'result': fields.String,\n",
    "})"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ns.route('/')\n",
    "class PhishingApi(Resource):\n",
    "\n",
    "    @api.doc(parser=parser)\n",
    "    @api.marshal_with(resource_fields)\n",
    "    def get(self):\n",
    "        args = parser.parse_args()\n",
    "        \n",
    "        plot = args['plot']\n",
    "        print(plot)\n",
    "        \n",
    "        return {\n",
    "         \"result\": valores(plot)\n",
    "        }, 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "   WARNING: This is a development server. Do not use it in a production deployment.\n",
      "   Use a production WSGI server instead.\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://0.0.0.0:5000/ (Press CTRL+C to quit)\n",
      "127.0.0.1 - - [17/Jul/2020 19:18:53] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
      "127.0.0.1 - - [17/Jul/2020 19:18:54] \"\u001b[37mGET /swagger.json HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a serial killer decides to teach the secrets of his satisfying career to a video store clerk .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\jcoronelv\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\base.py:154: UserWarning: Loky-backed parallel loops cannot be nested below threads, setting n_jobs=1\n",
      "  n_jobs = min(effective_n_jobs(n_jobs), n_estimators)\n",
      "127.0.0.1 - - [17/Jul/2020 19:20:23] \"\u001b[37mGET /valores/?plot=a%20serial%20killer%20decides%20to%20teach%20the%20secrets%20of%20his%20satisfying%20career%20to%20a%20video%20store%20clerk%20. HTTP/1.1\u001b[0m\" 200 -\n"
     ]
    }
   ],
   "source": [
    "app.run(debug=True, use_reloader=False, host='0.0.0.0', port=5000)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "P3_MovieGenrePrediction_Redes_11jul_Champ.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
